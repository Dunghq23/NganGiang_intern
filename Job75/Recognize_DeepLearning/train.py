# -*- coding: utf-8 -*-
"""Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/160QMj4y0kxbbCpk7HEtLwMdK2QIJ0lUy
"""

!pip install tensorflow keras
!pip install h5py
!pip install numpy opencv-python matplotlib scipy

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import classification_report, confusion_matrix

# Đường dẫn đến thư mục chứa dữ liệu trên Google Drive
data_dir = "/content/drive/MyDrive/NGANGIANGINTERNSHIP/Job74/DL/labeled_images"  # Cập nhật đường dẫn nếu cần

# Tham số cho mô hình
img_width, img_height = 64, 64
batch_size = 32
epochs = 50
grayscale = True

# Load dữ liệu
def load_data(data_dir):
    images = []
    labels = []
    class_dirs = sorted(os.listdir(data_dir))
    label_to_idx = {class_name: idx for idx, class_name in enumerate(class_dirs)}

    for class_name in class_dirs:
        class_path = os.path.join(data_dir, class_name)
        if not os.path.isdir(class_path):
            continue

        for img_file in os.listdir(class_path):
            img_path = os.path.join(class_path, img_file)
            try:
                if grayscale:
                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
                    img = cv2.resize(img, (img_width, img_height))
                    img = np.expand_dims(img, axis=-1)
                else:
                    img = cv2.imread(img_path)
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    img = cv2.resize(img, (img_width, img_height))

                if img is not None:
                    images.append(img)
                    labels.append(label_to_idx[class_name])
            except Exception as e:
                print(f"Lỗi khi đọc ảnh {img_path}: {e}")

    return np.array(images), np.array(labels), label_to_idx

# Tải dữ liệu
images, labels, label_to_idx = load_data(data_dir)
idx_to_label = {v: k for k, v in label_to_idx.items()}
num_classes = len(label_to_idx)

print(f"Số lượng ảnh: {len(images)}")
print(f"Số lượng nhãn: {num_classes}")

# Chuẩn hóa dữ liệu
images = images.astype('float32') / 255.0
print(f"Kích thước tập dữ liệu: {images.shape}")

# Chia tập dữ liệu
X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.3, stratify=labels, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes)

print(f"Kích thước tập huấn luyện: {X_train.shape}")
print(f"Kích thước tập kiểm định: {X_val.shape}")
print(f"Kích thước tập kiểm thử: {X_test.shape}")

# # Tính class weights
# class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)
# class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}

# print(f"Class weights: {class_weights_dict}")

# Đảm bảo y_train là một mảng numpy
y_train = np.array(y_train)

# Tính toán class weights
class_weights =class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)

# Chuyển thành dictionary
class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}

print(f"Class weights: {class_weights_dict}")

# Data Augmentation
datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    fill_mode='nearest'
)

print("Datagen", datagen)

# Xây dựng mô hình CNN
input_shape = (img_width, img_height, 1) if grayscale else (img_width, img_height, 3)

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),
    BatchNormalization(),
    MaxPooling2D(2, 2),
    Dropout(0.2),

    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(2, 2),
    Dropout(0.3),

    Conv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(2, 2),
    Dropout(0.4),

    Flatten(),
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks
checkpoint = ModelCheckpoint("/content/drive/MyDrive/NGANGIANGINTERNSHIP/Job74/DL/best_model2.h5", monitor='val_accuracy', save_best_only=True, mode='max')
early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

callbacks = [checkpoint, early_stop, reduce_lr]

# Huấn luyện
history = model.fit(
    datagen.flow(X_train, y_train_onehot, batch_size=batch_size),
    epochs=epochs,
    validation_data=(X_val, y_val_onehot),
    class_weight=class_weights_dict,
    callbacks=callbacks
)

# Đánh giá mô hình
_, test_acc = model.evaluate(X_test, y_test_onehot)
print(f"Độ chính xác trên tập kiểm tra: {test_acc*100:.2f}%")

# Lưu mô hình
model.save("/content/drive/MyDrive/NGANGIANGINTERNSHIP/Job74/DL/model_nhan_dang_bien_so_final_2.h5")

# Vẽ đồ thị kết quả
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Độ chính xác huấn luyện')
plt.plot(history.history['val_accuracy'], label='Độ chính xác kiểm định')
plt.legend()
plt.title('Độ chính xác qua các epoch')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Mất mát huấn luyện')
plt.plot(history.history['val_loss'], label='Mất mát kiểm định')
plt.legend()
plt.title('Mất mát qua các epoch')
plt.show()

# Confusion Matrix
y_pred = np.argmax(model.predict(X_test), axis=1)
y_true = np.argmax(y_test_onehot, axis=1)
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=idx_to_label.values(), yticklabels=idx_to_label.values())
plt.xlabel('Dự đoán')
plt.ylabel('Thực tế')
plt.title('Confusion Matrix')
plt.show()

